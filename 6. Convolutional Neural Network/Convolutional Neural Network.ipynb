{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, BatchNormalization, Activation, Input, \\\n",
    "    Conv1D, MaxPool1D, Flatten, Concatenate, Add\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "from numpy.random import seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name: a file name, type: str\n",
    "    return a list of ids, a list of reviews, a list of labels\n",
    "    https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_name)\n",
    "\n",
    "    return df[\"id\"], df[\"text\"], df[\"label\"]\n",
    "\n",
    "def load_labels(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name: a file name, type: str\n",
    "    return a list of labels\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_name)[\"label\"]\n",
    "\n",
    "def write_predictions(file_name, pred):\n",
    "    df = pd.DataFrame(zip(range(len(pred)), pred))\n",
    "    df.columns = [\"id\", \"label\"]\n",
    "    df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def stem(tokens):\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "    \n",
    "def filter_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feats_dict(feats, min_freq=-1, max_freq=-1, max_size=-1):\n",
    "    # count all features\n",
    "    feat_cnt = Counter(feats) # [\"text\", \"text\", \"mine\"] --> {\"text\": 2, \"mine\": 1}\n",
    "    if max_size > 0 and min_freq == -1 and max_freq == -1:\n",
    "        valid_feats = [\"<pad>\", \"<unk>\"] + [f for f, cnt in feat_cnt.most_common(max_size-2)]\n",
    "    else:\n",
    "        valid_feats = [\"<pad>\", \"<unk>\"]\n",
    "        for f, cnt in feat_cnt.most_common():\n",
    "            if (min_freq == -1 or cnt >= min_freq) and \\\n",
    "                (max_freq == -1 or cnt <= max_freq):\n",
    "                valid_feats.append(f)\n",
    "    if max_size > 0 and len(valid_feats) > max_size:\n",
    "        valid_feats = valid_feats[:max_size]\n",
    "    print(\"Size of features:\", len(valid_feats))\n",
    "    \n",
    "    # build a mapping from features to indices\n",
    "    feats_dict = dict(zip(valid_feats, range(len(valid_feats))))\n",
    "    return feats_dict\n",
    "\n",
    "def get_index_vector(feats, feats_dict, max_len):\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(max_len, dtype=np.int64)\n",
    "    for i, f in enumerate(feats):\n",
    "        if i == max_len:\n",
    "            break\n",
    "        # get the feature index, return 1 (<unk>) if the feature is not existed\n",
    "        f_idx = feats_dict.get(f, 1)\n",
    "        vector[i] = f_idx\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN consists of three parts:\n",
    "1. The word representation part: the word embedding layer\n",
    "2. The conv-pool part includes multiple convolutional layers with different kernel sizes, maxpooling layers to aggregate extracted features\n",
    "3. The fully connected part utilizes a multi-layer perceptron to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_CNN(input_length, vocab_size, embedding_size,\n",
    "              hidden_size, output_size,\n",
    "              kernel_sizes, num_filters, num_mlp_layers,\n",
    "              padding=\"valid\",\n",
    "              strides=1,\n",
    "              activation=\"relu\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"SGD\",\n",
    "              learning_rate=0.1,\n",
    "              metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    :param input_length: the maximum length of sentences, type: int\n",
    "    :param vocab_size: the vacabulary size, type: int\n",
    "    :param embedding_size: the dimension of word representations, type: int\n",
    "    :param hidden_size: the dimension of the hidden states, type: int\n",
    "    :param output_size: the dimension of the prediction, type: int\n",
    "    :param kernel_sizes: the kernel sizes of convolutional layers, type: list\n",
    "    :param num_filters: the number of filters for each kernel, type: int\n",
    "    :param num_mlp_layers: the number of layers of the MLP, type: int\n",
    "    :param padding: the padding method in convolutional layers, type: str\n",
    "    :param strides: the strides in convolutional layers, type: int\n",
    "    :param activation: the activation type, type: str\n",
    "    :param dropout_rate: the probability of dropout, type: float\n",
    "    :param batch_norm: whether to enable batch normalization, type: bool\n",
    "    :param l2_reg: the weight for the L2 regularizer, type: str\n",
    "    :param loss: the training loss, type: str\n",
    "    :param optimizer: the optimizer, type: str\n",
    "    :param learning_rate: the learning rate for the optimizer, type: float\n",
    "    :param metric: the metric, type: str\n",
    "    return a CNN for text classification,\n",
    "    # activation document: https://keras.io/activations/\n",
    "    # dropout document: https://keras.io/layers/core/#dropout\n",
    "    # embedding document: https://keras.io/layers/embeddings/#embedding\n",
    "    # convolutional layers document: https://keras.io/layers/convolutional\n",
    "    # pooling layers document: https://keras.io/layers/pooling/\n",
    "    # batch normalization document: https://keras.io/layers/normalization/\n",
    "    # losses document: https://keras.io/losses/\n",
    "    # optimizers document: https://keras.io/optimizers/\n",
    "    # metrics document: https://keras.io/metrics/\n",
    "    \"\"\"\n",
    "    x = Input(shape=(input_length,))\n",
    "    \n",
    "    ################################\n",
    "    ###### Word Representation #####\n",
    "    ################################\n",
    "    # word representation layer\n",
    "    emb = Embedding(input_dim=vocab_size,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=input_length,\n",
    "                    embeddings_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=0))(x)\n",
    "    \n",
    "    ################################\n",
    "    ########### Conv-Pool ##########\n",
    "    ################################\n",
    "    # convolutional and pooling layers\n",
    "    cnn_results = list()\n",
    "    for kernel_size in kernel_sizes:\n",
    "        # add convolutional layer\n",
    "        conv = Conv1D(filters=num_filters,\n",
    "                      kernel_size=(kernel_size,),\n",
    "                      padding=padding,\n",
    "                      strides=strides)(emb)\n",
    "        # add batch normalization layer\n",
    "        if batch_norm:\n",
    "            conv = BatchNormalization()(conv)\n",
    "        # add activation\n",
    "        conv = Activation(activation)(conv)\n",
    "        # add max-pooling\n",
    "        maxpool = MaxPool1D(pool_size=(input_length-kernel_size)//strides+1)(conv)\n",
    "        cnn_results.append(Flatten()(maxpool))\n",
    "    \n",
    "    ################################\n",
    "    ##### Fully Connected Layer ####\n",
    "    ################################\n",
    "    h = Concatenate()(cnn_results) if len(kernel_sizes) > 1 else cnn_results[0]\n",
    "    h = Dropout(dropout_rate, seed=0)(h)\n",
    "    # multi-layer perceptron\n",
    "    for i in range(num_mlp_layers-1):\n",
    "        new_h = Dense(hidden_size,\n",
    "                      kernel_regularizer=keras.regularizers.l2(l2_reg))(h)\n",
    "        # add batch normalization layer\n",
    "        if batch_norm:\n",
    "            new_h = BatchNormalization()(new_h)\n",
    "        # add skip connection\n",
    "        if i == 0:\n",
    "            h = new_h\n",
    "        else:\n",
    "            h = Add()([h, new_h])\n",
    "        # add activation\n",
    "        h = Activation(activation)(h)\n",
    "    y = Dense(output_size,\n",
    "              activation=\"softmax\")(h)\n",
    "    \n",
    "    # set the loss, the optimizer, and the metric\n",
    "    if optimizer == \"SGD\":\n",
    "        optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    elif optimizer == \"RMSprop\":\n",
    "        optmizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == \"Adam\":\n",
    "        optmizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    model = Model(x, y)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try CNN for Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"data/train.csv\"\n",
    "test_file = \"data/test.csv\"\n",
    "ans_file = \"data/ans.csv\"\n",
    "min_freq = 3\n",
    "\n",
    "# load data\n",
    "train_ids, train_texts, train_labels = load_data(train_file)\n",
    "test_ids, test_texts, _ = load_data(test_file)\n",
    "test_labels = load_labels(ans_file)\n",
    "\n",
    "# extract features\n",
    "train_tokens = [tokenize(text) for text in train_texts]\n",
    "test_tokens = [tokenize(text) for text in test_texts]\n",
    "\n",
    "train_stemmed = [stem(tokens) for tokens in train_tokens]\n",
    "test_stemmed = [stem(tokens) for tokens in test_tokens]\n",
    "\n",
    "train_feats = [filter_stopwords(tokens) for tokens in train_stemmed]\n",
    "test_feats = [filter_stopwords(tokens) for tokens in test_stemmed]\n",
    "\n",
    "# build a mapping from features to indices\n",
    "feats_dict = get_feats_dict(\n",
    "    chain.from_iterable(train_feats),\n",
    "    min_freq=min_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "\n",
    "# build the feats_matrix\n",
    "# convert each example to a index vector, and then stack vectors as a matrix\n",
    "train_feats_matrix = np.vstack(\n",
    "    [get_index_vector(f, feats_dict, max_len) for f in train_feats])\n",
    "test_feats_matrix = np.vstack(\n",
    "    [get_index_vector(f, feats_dict, max_len) for f in test_feats])\n",
    "\n",
    "# convert labels to label_matrix\n",
    "num_classes = max(train_labels)\n",
    "# convert each label to a ont-hot vector, and then stack vectors as a matrix\n",
    "train_label_matrix = keras.utils.to_categorical(train_labels-1, num_classes=num_classes)\n",
    "test_label_matrix = keras.utils.to_categorical(test_labels-1, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a CNN whose embedding_size is 100, kernel_sizes are 1,2,3,4, num_filters is 100, hidden_size is 100, num_mlp_layers is 3, and activation is ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "seed(0)\n",
    "tf.random.set_random_seed(0)\n",
    "\n",
    "model = build_CNN(input_length=max_len, vocab_size=len(feats_dict),\n",
    "                  embedding_size=100, hidden_size=100, output_size=num_classes,\n",
    "                  kernel_sizes=[1,2,3,4], num_filters=100, num_mlp_layers=3,\n",
    "                  activation=\"relu\")\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"cnn1_weights.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=0,\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "cnn_history = model.fit(train_feats_matrix, train_label_matrix,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=100, batch_size=100, verbose=0,\n",
    "                    callbacks=[checkpointer])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"cnn1_weights.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_feats_matrix, train_label_matrix,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(test_feats_matrix, test_label_matrix,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid the instability and over-training, we add dropout_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(0)\n",
    "tf.random.set_random_seed(0)\n",
    "\n",
    "model = build_CNN(input_length=max_len, vocab_size=len(feats_dict),\n",
    "                  embedding_size=100, hidden_size=100, output_size=num_classes,\n",
    "                  kernel_sizes=[1,2,3,4], num_filters=100, num_mlp_layers=3,\n",
    "                  activation=\"relu\",\n",
    "                  dropout_rate=0.3, l2_reg=0.05, batch_norm=True)\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"cnn2_weights.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=0,\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "history = model.fit(train_feats_matrix, train_label_matrix,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=100, batch_size=100, verbose=0,\n",
    "                    callbacks=[checkpointer])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"cnn2_weights.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_feats_matrix, train_label_matrix,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(test_feats_matrix, test_label_matrix,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
