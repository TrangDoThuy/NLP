{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from scipy.sparse import coo_matrix\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\trang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\trang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download ('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name: a file name, type: str\n",
    "    return a list of ids, a list of reviews, a list of labels\n",
    "    https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_name)\n",
    "\n",
    "    return df['id'], df[\"text\"], df['label']\n",
    "\n",
    "def load_labels(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name: a file name, type: str\n",
    "    return a list of labels\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_name)['label']\n",
    "\n",
    "def write_predictions(file_name, pred):\n",
    "    df = pd.DataFrame(zip(range(len(pred)), pred))\n",
    "    df.columns = [\"id\", \"label\"]\n",
    "    df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "def stem(tokens):\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "def n_gram(tokens, n=1):\n",
    "    if n == 1:\n",
    "        return tokens\n",
    "    else:\n",
    "        results = list()\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n",
    "            results.append(\" \".join(tokens[i:i+n]))\n",
    "        return results\n",
    "def filter_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot_vector(feats, feats_dict):\n",
    "    \"\"\"\n",
    "    :param feats: a list of features, type: list\n",
    "    :param feats_dict: a dict from features to indices, type: dict\n",
    "    return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(len(feats_dict), dtype=np.float)\n",
    "    for f in feats:\n",
    "        # get the feature index, return -1 if the feature is not existed\n",
    "        f_idx = feats_dict.get(f, -1)\n",
    "        if f_idx != -1:\n",
    "            # set the corresponding element as 1\n",
    "            vector[f_idx] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Naive Bayes algorithm\n",
    "For more information, please check the web page:https://scikit-learn.org/stable/modules/naive_bayes.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf0 = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"data/train.csv\"\n",
    "test_file = \"data/test.csv\"\n",
    "ans_file = \"data/answer.csv\"\n",
    "pred_file = \"data/pred.csv\"\n",
    "\n",
    "# load data\n",
    "train_ids, train_texts, train_labels = load_data(train_file)\n",
    "test_ids, test_texts, _ = load_data(test_file)\n",
    "test_labels = load_labels(ans_file)\n",
    "\n",
    "# extract features\n",
    "\n",
    "# tokenization\n",
    "train_tokens = [tokenize(text) for text in train_texts] \n",
    "test_tokens = [tokenize(text) for text in test_texts]\n",
    "\n",
    "# stemming\n",
    "train_stemmed = [stem(tokens) for tokens in train_tokens]\n",
    "test_stemmed = [stem(tokens) for tokens in test_tokens]\n",
    "\n",
    "# n-gram\n",
    "train_2_gram = [n_gram(tokens, 2) for tokens in train_stemmed]\n",
    "train_3_gram = [n_gram(tokens, 3) for tokens in train_stemmed]\n",
    "train_4_gram = [n_gram(tokens, 4) for tokens in train_stemmed]\n",
    "\n",
    "test_2_gram = [n_gram(tokens, 2) for tokens in test_stemmed]\n",
    "test_3_gram = [n_gram(tokens, 3) for tokens in test_stemmed]\n",
    "test_4_gram = [n_gram(tokens, 4) for tokens in test_stemmed]\n",
    "\n",
    "\n",
    "# remove stopwords\n",
    "# the input should be the stemmed tokens and the output is a cleanner token list\n",
    "train_stemmed = [filter_stopwords(tokens) for tokens in train_stemmed]\n",
    "test_stemmed = [filter_stopwords(tokens) for tokens in test_stemmed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only use those features which occur more than 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a set containing each unique feature which has appeared more than 10 times in the training set\n",
    "feats_set = set()\n",
    "\n",
    "# build a Counter for stemmed features, e.g., {\"text\": 2, \"mine\": 1}\n",
    "stemmed_feat_cnt = Counter()\n",
    "\n",
    "for feats in train_stemmed:\n",
    "    stemmed_feat_cnt.update(feats)\n",
    "\n",
    "# add those stem features which occurs more than 10 times into the feature set.\n",
    "feats_set.update([f for f, cnt in stemmed_feat_cnt.items() if cnt > 10]) \n",
    "    \n",
    "\n",
    "# build a Counter for 2-gram features\n",
    "bi_gram_feat_cnt = Counter()\n",
    "for feats in train_2_gram:\n",
    "    bi_gram_feat_cnt.update(feats)\n",
    "\n",
    "# add those 2-gram features which occurs more than 10 times into the feature set.\n",
    "feats_set.update([f for f, cnt in bi_gram_feat_cnt.items() if cnt > 10]) \n",
    "\n",
    "\n",
    "# build a Counter for 3-gram features\n",
    "tri_gram_feat_cnt = Counter()\n",
    "\n",
    "for feats in train_3_gram:\n",
    "    tri_gram_feat_cnt.update(feats)\n",
    "\n",
    "# add those 3-gram features which occurs more than 10 times into the feature set.\n",
    "feats_set.update([f for f, cnt in tri_gram_feat_cnt.items() if cnt > 10]) \n",
    "\n",
    "\n",
    "# first, build a Counter for 4-gram features\n",
    "four_gram_feat_cnt = Counter()\n",
    "\n",
    "for feats in train_4_gram:\n",
    "    four_gram_feat_cnt.update(feats)\n",
    "    \n",
    "# add those 4-gram features which occurs more than 10 times into the feature set.\n",
    "feats_set.update([f for f, cnt in four_gram_feat_cnt.items() if cnt > 10]) \n",
    "\n",
    "\n",
    "print(\"Size of features:\", len(feats_set))\n",
    "\n",
    "# build the feature dict mapping each feature to its index \n",
    "feats_dict = dict(zip(feats_set, range(len(feats_set))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the feature list\n",
    "train_feats = list()\n",
    "for i in range(len(train_ids)):\n",
    "    # concatenate the stemmed token list and all n-gram list together\n",
    "    train_feats.append(train_stemmed[i] + train_2_gram[i] + train_3_gram[i] + train_4_gram[i])\n",
    "test_feats = list()\n",
    "for i in range(len(test_ids)):\n",
    "    # concatenate the stemmed token list and all n-gram list together\n",
    "    test_feats.append(test_stemmed[i] + test_2_gram[i]+ test_3_gram[i] + test_4_gram[i])\n",
    "\n",
    "\n",
    "\n",
    "# build the feats_matrix\n",
    "# We first convert each example to a ont-hot vector, and then stack vectors as a matrix. Afterwards,\n",
    "# we save this feature matirx in a COO sparse matrix format to reduce memory consumption.\n",
    "# See https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html and \n",
    "# https://en.wikipedia.org/wiki/Sparse_matrix#Coordinate_list_(COO) for details.\n",
    "\n",
    "train_feats_matrix = coo_matrix(np.vstack([get_onehot_vector(f, feats_dict) for f in train_feats]))\n",
    "test_feats_matrix = coo_matrix(np.vstack([get_onehot_vector(f, feats_dict) for f in test_feats]))\n",
    "\n",
    "# Fit the feature matrix and labels to train the classifier.\n",
    "# Since the classifier can only process matrices in the dense format,\n",
    "# we use toarray() function to get the dense representation of the sparse \n",
    "# matrix before passing it to the classifier\n",
    "clf0.fit(train_feats_matrix.toarray(), train_labels.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the classifier on train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the predictions of the classifier\n",
    "train_pred = clf0.predict(train_feats_matrix.toarray())\n",
    "test_pred = clf0.predict(test_feats_matrix.toarray())\n",
    "\n",
    "#Compute accuracy scores\n",
    "train_score = accuracy_score(train_labels.values, train_pred)\n",
    "test_score = accuracy_score(test_labels.values, test_pred)\n",
    "print(\"training accuracy\", train_score)\n",
    "print(\"test accuracy\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation and Ensemble\n",
    "\n",
    "We can use cross validation with the ensemble technique to reduce overfitting as well as the randomness issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_fold document: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
    "n_fold = 3\n",
    "np.random.seed(0)\n",
    "# create the n-fold generator\n",
    "skf = StratifiedKFold(n_fold, shuffle=True)\n",
    "\n",
    "clfs_1 = list()\n",
    "valid_acc_list = list()\n",
    "for k, (train_idx, valid_idx) in enumerate(\n",
    "    skf.split(train_feats_matrix.toarray(), train_labels)):\n",
    "    # build the classifier and train\n",
    "    clf = GaussianNB()\n",
    "    \n",
    "    clf.fit(train_feats_matrix.toarray()[train_idx], train_labels.values[train_idx])\n",
    "    \n",
    "    #Get the predictions of the classifier\n",
    "    train_pred = clf.predict(train_feats_matrix.toarray()[train_idx])\n",
    "    valid_pred = clf.predict(train_feats_matrix.toarray()[valid_idx])\n",
    "\n",
    "    #Compute accuracy scores\n",
    "    train_score = accuracy_score(train_labels.values[train_idx], train_pred)\n",
    "    valid_score = accuracy_score(train_labels.values[valid_idx], valid_pred)\n",
    "    \n",
    "    print(\"training accuracy\", train_score)\n",
    "    print(\"validation accuracy\", valid_score)\n",
    "    \n",
    "    clfs_1.append(clf)\n",
    "    valid_acc_list.append(valid_score)\n",
    "    \n",
    "print('Average validation score: ', sum(valid_acc_list)/len(valid_acc_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
