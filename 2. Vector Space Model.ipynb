{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing piplines:\n",
    "Text denoising -> text normalization -> text standarization\n",
    "\n",
    "In this part, we will conduct text standarization using vector space model.\n",
    "\n",
    "1. Stop words, ngrams, the whole pipeline of text preprocessing.\n",
    "2. Bag-of-word representation\n",
    "3. Term weighting: \n",
    "    - Term frequency\n",
    "    - Inverse document frequency\n",
    "    - TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Stop words, ngrams, the whole pipeline of text preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords's intuition: Not all words are informative\n",
    "\n",
    "- Remove such words to reduce vocabulary size\n",
    "- Stopwords do not have universal definition\n",
    "- Risk: we may break the original meaning and structure of text when we remove some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\trang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doesn', 'me', 'd', 'but', 'into', 'have', 'if', 'no', 'this', 'while', 'mightn', 'needn', 'they', 'most', 'up', 'which', 'during', 'there', 'by', 'she', 'any', 'that', 'had', \"wasn't\", 'those', 'than', 'of', 'under', 'don', 'from', \"couldn't\", 'herself', 'you', 'won', 'ours', 'yourself', 'above', \"weren't\", 'so', 'with', 'hasn', 'few', 'should', 'ma', 'further', 'been', 'wouldn', 'these', \"it's\", 'couldn', 'didn', \"needn't\", 'its', 'whom', 'below', 'our', 'myself', 'only', \"that'll\", 'hers', 'or', 'wasn', 'as', 'themselves', \"shan't\", 'itself', \"you'd\", 'be', 'do', 'against', \"didn't\", 'about', 'then', 't', 'between', 'nor', 'am', 'being', 'out', 'can', 'at', 'hadn', 'll', 'ain', \"mightn't\", 'off', 'until', 'to', 'both', 'weren', \"she's\", \"won't\", 'what', \"doesn't\", 'through', \"mustn't\", 'their', 'before', 'all', 'after', 'y', \"don't\", 'the', 'has', 'and', 'a', 'same', 'doing', \"you'll\", 'other', 'm', 'where', 'not', 'mustn', 'such', 'i', \"wouldn't\", 'when', \"shouldn't\", \"hasn't\", 'him', 'my', 'yourselves', 'is', 'because', 'o', \"you're\", 'does', \"haven't\", 'for', \"you've\", 'on', 'her', 'were', 'will', 'some', 'more', 'again', 'too', 's', 'once', 'who', 'shouldn', 'shan', 'it', 'are', 'having', 'how', 'own', 'here', 'why', 'aren', 'yours', 'very', 'just', 'theirs', 'over', 'now', 'each', \"aren't\", 'isn', \"should've\", 'haven', \"hadn't\", 'himself', \"isn't\", 'we', 'was', 'down', 'your', 'his', 're', 'in', 'he', 'did', 'them', 'an', 'ourselves', 've'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "\n",
    "def filter_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of filtered tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "\n",
    "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
      "['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
      "['text', 'mine', 'identifi', 'use', 'inform', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenize(\"Text mining is to identify useful information.\")\n",
    "print(tokens)\n",
    "\n",
    "tokens = stem(tokens)\n",
    "print(tokens)\n",
    "\n",
    "print(filter_stopwords(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single word is sometimes weakly expressive so that we can try n-gram for better representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram(tokens, n=1):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    :param n: the corresponding n-gram, type: int\n",
    "    return a list of n-gram tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'], 2\n",
    "    Output: ['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        return tokens\n",
    "    else:\n",
    "        results = list()\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n",
    "            results.append(\" \".join(tokens[i:i+n]))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n"
     ]
    }
   ],
   "source": [
    "bi_gram = n_gram(tokens, 2)\n",
    "print(bi_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text mine is', 'mine is to', 'is to identifi', 'to identifi use', 'identifi use inform', 'use inform .']\n"
     ]
    }
   ],
   "source": [
    "tri_gram = n_gram(tokens, 3)\n",
    "print(tri_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bag-of-words representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the words to numerical features.\n",
    "\n",
    "## One-hot vector\n",
    "A one-hot is a group of bits among which the legal combinations of values are only those with a single high (1) bit and all the others low (0).\n",
    "\n",
    "For an example sentence ```text mining is good```, we map all the words into indexes: map ```text``` to 0, ```mining``` to 1, ```is``` to 2, and ```good``` to 3.\n",
    "\n",
    "Then the one hot vector for ```text``` is ```[1, 0, 0, 0]```. For ```mining``` it is ```[0, 1, 0, 0]```, and so on.\n",
    "\n",
    "## Bag of word (BOW)\n",
    "The BOW vector of a sentence is the sum of all the one-hot vectors.\n",
    "\n",
    "For ```text mining is good```, the BOW representation is ```[1, 1, 1, 1]```.\n",
    "\n",
    "For ```text mining good```, the BOW representation is ```[1, 1, 0, 1]```.\n",
    "\n",
    "For ```text mining is good mining```, the BOW representation is ```[1, 2, 1, 1]```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_vocabulary_mapping(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of word tokens type: list\n",
    "    \n",
    "    return vocab_dict: a dict from words to indices, type: dict\n",
    "    \"\"\"\n",
    "    vocab_dict = {}\n",
    "    for token in tokens:\n",
    "        if not token in vocab_dict:\n",
    "            vocab_dict[token] = len(vocab_dict)\n",
    "    return vocab_dict\n",
    "\n",
    "def get_bow_vector(tokens, vocab_dict):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokenized words, type: list\n",
    "    :param vocab_dict: a dict from words to indices, type: dict\n",
    "    \n",
    "    return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(len(vocab_dict), dtype=np.float)\n",
    "    for f in tokens:\n",
    "        # get the feature index, return -1 if the feature is not existed\n",
    "        f_idx = vocab_dict.get(f, -1)\n",
    "        if f_idx != -1:\n",
    "            # set the corresponding element as 1\n",
    "            vector[f_idx] = vector[f_idx] + 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocabulary_mapping([\"text\", \"mining\", \"is\", \"good\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 0, 'mining': 1, 'is': 2, 'good': 3}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 1. 1.]\n",
      "[1. 0. 1. 1.]\n",
      "[1. 0. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "tokens_1 = tokenize(\"text mining is good mining.\")\n",
    "tokens_2 = tokenize(\"text is good\")\n",
    "tokens_3 = tokenize(\"good text is good\")\n",
    "print(get_bow_vector(tokens_1, vocab))\n",
    "print(get_bow_vector(tokens_2, vocab))\n",
    "print(get_bow_vector(tokens_3, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bow(corpus):\n",
    "    \"\"\"\n",
    "        :param corpus: a list of strings (type: list)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def vectorize(sentence, vocab):\n",
    "        \"\"\"\n",
    "           :param sentence: a string (type:str)\n",
    "           :param vocab: vocabulary (type:list) \n",
    "           \n",
    "           count function of a list is to count the time of occurance in a list\n",
    "           :return BOW vector\n",
    "        \"\"\"\n",
    "        return [sentence.split().count(i) for i in vocab]\n",
    "\n",
    "    vectorized_corpus = []\n",
    "    vocab = sorted(set([token for doc in corpus for token in doc.lower().split()]))\n",
    "    for sent in corpus:\n",
    "        vectorized_corpus.append((sent, vectorize(sent, vocab)))\n",
    "    return vectorized_corpus, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('text mining is good mining', [1, 1, 2, 1]), ('text is good', [1, 1, 0, 1]), ('good text is good', [2, 1, 0, 1])]\n",
      "['good', 'is', 'mining', 'text']\n"
     ]
    }
   ],
   "source": [
    "all_sents = [\"text mining is good mining\",\n",
    "\"text is good\",\n",
    "\"good text is good\"]\n",
    "corpus_bow, vocab = calculate_bow(all_sents)\n",
    "print(corpus_bow)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def cosine_sim(u,v):\n",
    "    return np.dot(u,v) / (sqrt(np.dot(u,u)) * sqrt(np.dot(v,v)))\n",
    "\n",
    "def print_similarity(corpus):\n",
    "    \"\"\"\n",
    "    Print pairwise similarities\n",
    "    \"\"\"\n",
    "    for sentx in corpus:\n",
    "        for senty in corpus:\n",
    "            print(\"{:.4f}\".format(cosine_sim(sentx[1], senty[1])), end=\" \")\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000 0.6547 0.6172 \n",
      "0.6547 1.0000 0.9428 \n",
      "0.6172 0.9428 1.0000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_similarity(corpus_bow)\n",
    "# [\"text mining is good mining\",\n",
    "# \"text is good\",\n",
    "# \"good text is good\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Term weighting\n",
    "Term frequency: number of times term ***t*** appearing in document ***d***\n",
    "\n",
    "Inverse document frequency: total number of docs in collection over number of docs containing term t\n",
    "\n",
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_freq(freq_dict, term):\n",
    "    \"\"\"\n",
    "        :param freq_dict (type:dict): a dict variable whose key is the word and value is the frequency.\n",
    "            e.g., dict([\"text\":1, \"mining\":1, \"is\":1])\n",
    "        :param term (type:str): the candidate word to calculate TF\n",
    "        \n",
    "        :returns, the TF score of a certain word\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return freq_dict[term] / float(sum(freq_dict.values()))\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "\n",
    "def inverse_doc_freq(freq_dict_list, term):\n",
    "    \"\"\"\n",
    "        :param freq_dict_list (type: list): a list of freq_dict \n",
    "        :param term (type:str): the candidate word to calculate TF\n",
    "        \n",
    "        :returns, the IDF of a certain word\n",
    "    \"\"\"\n",
    "    try:\n",
    "        unique_docs = sum([1 for i,_ in enumerate(freq_dict_list) if freq_dict_list[i][term] > 0])\n",
    "        return float(len(freq_dict_list)) / unique_docs\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow representation: ('good text is good', [2, 1, 0, 1])\n",
      "frequency dict: {'good': 2, 'is': 1, 'mining': 0, 'text': 1}\n"
     ]
    }
   ],
   "source": [
    "# sentence: good text is good\n",
    "current_bow = corpus_bow[2]\n",
    "print(\"bow representation:\", current_bow)\n",
    "doc_vec_dict = {k:v for k,v in zip(vocab, current_bow[1])}\n",
    "print(\"frequency dict:\", doc_vec_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF: good 0.5\n",
      "TF: text 0.25\n",
      "TF: is 0.25\n"
     ]
    }
   ],
   "source": [
    "print(\"TF: good\", term_freq(doc_vec_dict, \"good\"))\n",
    "print(\"TF: text\", term_freq(doc_vec_dict, \"text\"))\n",
    "print(\"TF: is\", term_freq(doc_vec_dict, \"is\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'good': 1, 'is': 1, 'mining': 2, 'text': 1}, {'good': 1, 'is': 1, 'mining': 0, 'text': 1}, {'good': 2, 'is': 1, 'mining': 0, 'text': 1}]\n"
     ]
    }
   ],
   "source": [
    "freq_dict_list = [{k:v for k,v in zip(vocab, vecs[1])} for vecs in corpus_bow]\n",
    "print(freq_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF: good 1.0\n",
      "IDF: text 1.0\n",
      "IDF: is 1.0\n",
      "IDF: mining 3.0\n"
     ]
    }
   ],
   "source": [
    "# all_sents = [\n",
    "# \"text mining is good mining\",\n",
    "# \"text is good\",\n",
    "# \"good text is good\"]\n",
    "print(\"IDF: good\", inverse_doc_freq(freq_dict_list, \"good\"))\n",
    "print(\"IDF: text\", inverse_doc_freq(freq_dict_list, \"text\"))\n",
    "print(\"IDF: is\", inverse_doc_freq(freq_dict_list, \"is\"))\n",
    "print(\"IDF: mining\", inverse_doc_freq(freq_dict_list, \"mining\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf(corpus_bow, vocab):\n",
    "    \"\"\"\n",
    "        :params corpus_bow(type: list): the BOW representation of the corpus\n",
    "        :params vocab (type:list): the list of vocab\n",
    "        \n",
    "        return the tf idf representation of the corpus\n",
    "    \"\"\"\n",
    "    word2id = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "    freq_dict_list = [{k:v for k,v in zip(vocab, i[1])} for i in corpus_bow]\n",
    "    tfidf_mat  =  np.zeros((len(freq_dict_list), len(vocab)), dtype=float)\n",
    "    for doc_id, doc in enumerate(freq_dict_list):\n",
    "        for term in doc:\n",
    "            term_id = word2id[term]\n",
    "            tf = term_freq(freq_dict_list[doc_id],term)\n",
    "            idf = inverse_doc_freq(freq_dict_list, term)\n",
    "            tfidf_mat[doc_id][term_id] = tf*idf\n",
    "\n",
    "    all_sents = [doc[0] for doc in corpus_bow]\n",
    "    corpus_tfidf = list(zip(all_sents, tfidf_mat))\n",
    "    return corpus_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_bow_tfidf = calculate_tfidf(corpus_bow, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('text mining is good mining', [1, 1, 2, 1]),\n",
       "  ('text mining is good mining', array([0.2, 0.2, 1.2, 0.2]))),\n",
       " (('text is good', [1, 1, 0, 1]),\n",
       "  ('text is good', array([0.33333333, 0.33333333, 0.        , 0.33333333]))),\n",
       " (('good text is good', [2, 1, 0, 1]),\n",
       "  ('good text is good', array([0.5 , 0.25, 0.  , 0.25])))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(corpus_bow, corpus_bow_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good', 'is', 'mining', 'text']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
