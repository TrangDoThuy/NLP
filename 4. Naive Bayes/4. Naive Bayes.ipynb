{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader\n",
    "Define function to load document id, raw text and labels from the input csv file. The input csv file (data/train.csv or data/test.csv) has the following 3 columns:\n",
    "\n",
    "1. id: document id\n",
    "2. text: document raw text\n",
    "3. label: document label (data/train.csv: one of the values in {1,2,3,4,5}; data/test.csv: -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name: a file name, type: str\n",
    "    return a list of ids, a list of documents, a list of labels\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_name)\n",
    "\n",
    "    return df['id'], df[\"text\"], df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to load document labels from the input csv file. The input csv file (data/answer.csv) has the following 2 columns:\n",
    "\n",
    "1. id: document id\n",
    "2. label: document label (one of the values in {1,2,3,4,5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name: a file name, type: str\n",
    "    return a list of labels\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_name)['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractor\n",
    "Define tokenization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for filtering stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of filtered tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     if token not in stopwords and not token.isnumeric():\n",
    "    #         results.append(token)\n",
    "    # return results\n",
    "\n",
    "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for building the Bag Of Word (BOW) representations of documents.\n",
    "\n",
    "Documentation of scipy lil matrix: https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html\n",
    "\n",
    "Documentation of scipy csr matrix: https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bagofwords(data, vocab_dict):\n",
    "    '''\n",
    "    :param data: a list of tokenized documents, type: list\n",
    "    :param vocab_dict: a mapping from words to indices, type: dict\n",
    "    return a BOW matrix in Compressed Sparse Row matrix format, type: scipy.sparse.csr_matrix\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    The BOW matrix is first constructed using Row-based list of lists sparse matrix (LIL) format.\n",
    "    LIL is a convenient format for constructing sparse matrices, as it supports flexible slicing, \n",
    "    and it is efficient to change to the matrix sparsity structure.\n",
    "    '''\n",
    "    \n",
    "    data_matrix = sparse.lil_matrix((len(data), len(vocab_dict)))\n",
    "\n",
    "    for i, doc in enumerate(data):\n",
    "        for word in doc:\n",
    "            word_idx = vocab_dict.get(word, -1)\n",
    "            if word_idx != -1:\n",
    "                data_matrix[i, word_idx] += 1\n",
    "                \n",
    "    '''\n",
    "    After constructing the BOW matrix on all input documents, we convert the matrix to Compressed Sparse \n",
    "    Row (CSR) format for fast arithmetic and matrix vector operations.\n",
    "    '''\n",
    "    data_matrix = data_matrix.tocsr()\n",
    "    \n",
    "    return data_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "Load document ids, raw texts, and labels from the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"data/train.csv\"\n",
    "test_file = \"data/test.csv\"\n",
    "ans_file = \"data/answer.csv\"\n",
    "\n",
    "\n",
    "train_ids, train_texts, train_labels = load_data(train_file)\n",
    "test_ids, test_texts, _ = load_data(test_file)\n",
    "test_labels = load_labels(ans_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of train set: {}\".format(len(train_ids)))\n",
    "print(\"Size of test set: {}\".format(len(test_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the raw texts in the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = [tokenize(text) for text in train_texts] \n",
    "test_tokens = [tokenize(text) for text in test_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words from the tokenized texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = [filter_stopwords(tokens) for tokens in train_tokens]\n",
    "test_tokens = [filter_stopwords(tokens) for tokens in test_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a vocabulary (i.e., a mapping from words to indices) on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a set data structure to hold all words appearing in the train set\n",
    "vocab = set()\n",
    "\n",
    "for i, doc in enumerate(train_tokens):# enumerate over each document in the train set\n",
    "    # enumerate over each word in the document\n",
    "    for word in doc:\n",
    "        # if this word has been added into the set before, \n",
    "        # then it will be ignored, otherwise, it will be \n",
    "        # added into the set.\n",
    "        vocab.add(word)\n",
    "        \n",
    "# create a dictionary from the set of words, where the\n",
    "# keys are word strings and the values are numerical indices\n",
    "vocab_dict = dict(zip(vocab, range(len(vocab))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Size of vocab: ', len(vocab_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the BOW matrices from the tokenized texts in train and test sets respectively, using the vocabulary and the get_bagofwords function defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_matrix = get_bagofwords(train_tokens, vocab_dict)\n",
    "test_data_matrix = get_bagofwords(test_tokens, vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Type of train_data_matrix: ', type(train_data_matrix))\n",
    "print('Type of test_data_matrix: ', type(test_data_matrix))\n",
    "print('Shape of train_data_matrix:', train_data_matrix.shape)\n",
    "print('Shape of test_data_matrix:', test_data_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "Define the following symbols:\n",
    "\n",
    "N_train = size of the train set\n",
    "\n",
    "N_test = size of the test set\n",
    "\n",
    "V = vocabulary size\n",
    "\n",
    "K = number of classes\n",
    "\n",
    "All indices of tensors are 0-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the size of the train set \n",
    "N_train = train_data_matrix.shape[0]\n",
    "\n",
    "# get the size of the test set \n",
    "N_test = test_data_matrix.shape[0]\n",
    "\n",
    "# get the vocabulary size\n",
    "V = len(vocab_dict)\n",
    "\n",
    "# get the number of classes\n",
    "K = max(train_labels)\n",
    "\n",
    "print('N_train: ', N_train)\n",
    "print('N_test: ', N_test)\n",
    "print('V: ', V)\n",
    "print('K: ', K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a utility function to normalize (with/without laplace smoothing) an input tensor over the first dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(P, smoothing_prior=0):\n",
    "    \"\"\"\n",
    "    e.g.\n",
    "    Input: [1,2,1,2,4]\n",
    "    Output: [0.1,0.2,0.1,0.2,0.4] (without laplace smoothing) or \n",
    "    [0.1333,0.2,0.1333,0.2,0.3333] (with laplace smoothing and the smoothing prior is 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the size of the first dimension\n",
    "    N = P.shape[0]\n",
    "    \n",
    "    # sum the tensor over the first dimension\n",
    "    # setting axis = 0 means the summation is performed over the first dimension\n",
    "    # setting keepdims=True means the reduced axes (i.e., the 0-th axis this case) \n",
    "    # are left in the result as dimensions with size one. With this option, the \n",
    "    # result will broadcast correctly against the input array.\n",
    "    \n",
    "    norm = np.sum(P, axis=0, keepdims=True)\n",
    "    \n",
    "    # perform the normalization by dividing the input tensor by the norm,\n",
    "    # and add smoothing prior in both the numerator and the denominator.\n",
    "    return (P + smoothing_prior) / (norm + smoothing_prior*N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a utility function to compute the accuracy score given the ground truth labels and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pre):\n",
    "    acc = accuracy_score(y_true, y_pre)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Given:\n",
    "\n",
    "1. the training labels (1-d array of shape (N_train,));\n",
    "\n",
    "2. the BOW matrix of training documents (scipy.sparse.csr_matrix of shape (N_train,V)),\n",
    "\n",
    "the training of Naive Bayes classifier is to compute the following two probabilities:\n",
    "\n",
    "1. prior: P(y) (an 1-d array with shape (K,), where the entry at position [l] is the is the prior probability of label l+1);\n",
    "\n",
    "2. likelihood: P(x|y) (a matrix with shape (V,K), where the entry at position [i,l] is the probability of word i in the documents of label l+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matrix with shape (N_train,K), where the entry at\n",
    "# the position (i,j) is 1  \n",
    "# iff the (i+1)-th document belongs to (j+1)-th \n",
    "# class, otherwise it is 0\n",
    "\n",
    "data_label_onehot_matrix = np.zeros((N_train, K))\n",
    "\n",
    "for i, l in enumerate(train_labels):\n",
    "    # the (i+1)-th document has label l, so we \n",
    "    # set the entry at the position [i,l-1] to \n",
    "    # be 1\n",
    "    data_label_onehot_matrix[i, l-1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data_label_onehot_matrix.shape: ', data_label_onehot_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the frequencies of all labels in the train set by row-wise summation.\n",
    "\n",
    "Set axis = 0 so that the summation is across rows of the data_label_onehot_matrix.\n",
    "\n",
    "Set keepdims = False so that we can get an 1-d array of shape (K,) after the summation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_freq = np.sum(data_label_onehot_matrix, axis=0, keepdims=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute P(y) by normalizing the label frequencies with laplace smoothing, where the smoothing prior = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_y = normalize(label_freq, smoothing_prior=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a matrix word_freq of shape (V,K), where word_freq[i,j] is the frequency of word i in the documents of label (j+1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e104c9070673>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mword_freq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_label_onehot_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "word_freq = train_data_matrix.transpose().dot(data_label_onehot_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_freq[i,j] = the dot product of the following 2 vectors:\n",
    "\n",
    "1. The i-th row of train_data_matrix.transpose():\n",
    "\n",
    "2. The j-th column of data_label_onehot_matrix\n",
    "\n",
    "The i-th row of train_data_matrix.transpose() is the frequncies of word i in all documents in the train set (i.e., train_data_matrix.transpose()[i,k] is the frequency of word i in (k+1)-th document).\n",
    "\n",
    "The j-th column of data_label_onehot_matrix is a vector indicating whether each document in the train set has label (j+1) (i.e., data_label_onehot_matrix[k,j] = 1 if the (k+1)-th document has label (j+1), otherwise it is data_label_onehot_matrix[k,j] = 0)\n",
    "\n",
    "So the dot product of these two vectors is to sum over the frequencies of word i in all the train documents of label (j+1), which is the frequency of word i in the documents of label (j+1).\n",
    "\n",
    "Normalize the word_freq matrix over the rows (i.e., across all words in the vocabulary for each label) to get P(x|y) (a matrix with shape (V,K), where the entry at position [i,l] is the probability of word i in the documents of label l+1). The normalization is with laplace smoothing, where the smoothing prior = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_xy = normalize(word_freq,smoothing_prior=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log_P_dy = train_data_matrix.dot(log_P_xy)\n",
    "test_log_P_dy = test_data_matrix.dot(log_P_xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log_P = log_P_y + train_log_P_dy\n",
    "test_log_P = log_P_y + test_log_P_dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add 1 because labels strat from 1\n",
    "train_pred = np.argmax(train_log_P, axis=1) + 1\n",
    "test_pred = np.argmax(test_log_P, axis=1) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc= evaluate(train_labels, train_pred)\n",
    "print(\"Train Accuracy: {}\".format(train_acc))\n",
    "\n",
    "test_acc= evaluate(test_labels, test_pred)\n",
    "print(\"Test Accuracy: {}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
